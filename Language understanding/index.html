
<!DOCTYPE html>
<html>
	<head>
		<title>making sense of the richest data!</title>
		<!-- link to main stylesheet -->
				<link rel="stylesheet" type="text/css" href="MyFontsWebfontsKit.css">

		<link rel="stylesheet" type="text/css" href="/css/main.css">
		    <link rel="stylesheet" type="text/css" href="/css/prism.css">
		 <script type="text/javascript" src="/prism.js"></script>
  </head>
  <body>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<h1>
 Distance between two documents using Bag-of-Words representation
</h1>
	  <article>
		 <p>
			 During my little experiments, I realized that KNN classification on top of the Bag-of-Words 
		  representation exhibits different performances using two intimately related distances in \(R^{N}\): Cosine similarity   and 
		  Euclidian  distance. 
		  </p>
		  <p>
		  In brief, Cosine similarity normalizes the Euclidian distance between \(\mathbf{u}\) and \(\mathbf{v}\)
		  by their magnitudes. Simply consider two vectors \(\mathbf{u}=[1,\ 0,\ 0]\) and \(\mathbf{v}=[101,\ 0,\ 0]\). Then cosine_similarity(\(\mathbf{u}
		  , \mathbf{v})= 1\), and  euclidien_distance(\(\mathbf{u}
		  , \mathbf{v})= 100\)!
		  </p>
		  <p>
		  For comparing two  documents that are  represented by their word counts from a vocabulary, that is the Bag-of-Words 
		  representation, the choice of distance metric can have a dramatic effect. This is because two similar documents can have
			  different "magnitude", that is the number of occurrence  of a signature word, while they convey same semantic meaning.
			  
		  </p>

	  </article>
	  
</body>
</html>
